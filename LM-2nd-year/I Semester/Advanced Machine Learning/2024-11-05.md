# Model ensembles

After the training process, if could be useful to **ensemble** models: to do so we train multiple independent models. At test time we average their results. This typically leads to 2% extra performance. If models commit different types of error, then averaging the results could be very useful, because each model focuses on some aspects.

Instead of training independent models, it's more practical to use **multiple snapshots of the same model** during training.

![[Pasted image 20241105165716.png]]

Also changing schedule could be useful.
Instead of using actual parameter vector, we can **keep a moving average of the parameter vector** and use it at test time (**Polyak averaging**).

# Transfer learning

It's common to think that to train a CNN a lot of data is needed. However it's not necessary like that.
**Transfer Learning** consists in training a network on a large amount of data, then we freeze a part of the model and we only retrain (fine tune) smallest part of the model on limited amount of data. In practice, for freezing a part of a model, we can just set the learning rate to 0, so the weights are not updated.

![[Pasted image 20241105170800.png]]

To retrain the non frozen part we need to make distinctions w.r.t. number of samples and the nature of the dataset:

![[Pasted image 20241105172003.png]]

Transfer learning is **pervasive**: many models are based on a CNN trained on ImageNet to perform different tasks.
The real advantage of re-using a pre trained model is efficiency. Similar results could be obtained training from scratch but with a longer training time.

![[Pasted image 20241105172457.png]]

# AlexNet

Some recall to calculate the parameter of a CNN:
- Output channels equals the number of filters
- Output size = $\left( \frac{W-K}{2} +1 \right)$
- Memory size: $CH'W' \cdot 4B$
- Number of Floating point operations: $C_{out}H'W' \cdot C_{in}KK$

**AlexNet** is composed from the following **layers**:

![[Pasted image 20241105175934.png]]

Note that the flatten layer unravel the multiple-channel input into a vector, that will be the input of the fully connected part of the layer.
Alex has **three fully connected layer** . The entire architecture was build via trial and error approach, and it took a very long time.
Looking at the measures of memory, parameters and FLOP we notice a trend: Most of the memory is used in the first layers. FLOP are mostly in the convolutional part, and the majority of the parameters are in the fully connected part.

![[Pasted image 20241105180702.png]]

AlexNet won a competition where a **top 5 error** metric was employed: a prediction is considered wrong if the correct class does not appear in the 5 highest probability output of the model.

# VGG

The VGG group came up with some **design rules**: 
- All convolutional layer are 3 x 3 stride 1 pad 1.
- All pooling are 2 x 2 stride 2
- After pool, the number of channels is doubled

Networks gain some regular design, characterized by blocks.

![[Pasted image 20241105181427.png]]

Why those 3 x 3 convolutional layers? Because two 3 x 3 convolutional layers have the same receptive field as a single 5 x 5 layer, however **they have less parameters and takes less computation**.

Compared to AlexNet, VGG is a much bigger network: 25x memory usage, 2.3x params, 19.4x FLOP

# GoogLeNet (InceptionNet)

The main idea is to have a **deeper network**, with computational **efficiency**.
The main component is called **inception module**: this module is able to perform multiple convolution with different kernel size in parallel.
Another component is the **aggressive stem**: at the start the network aggressively **downsamples the input**: this allows to have a lighter workload at the start.

Then we have the inception module:

![[Pasted image 20241105182542.png]]

At the end there is **only fully connected layer**: 

![[Pasted image 20241105182634.png]]
There are also the **auxiliary classifier**: at that time there was no batch normalization, to make the model converge they added those classifier in the middle of the network. The idea is to have an extra loss from the classifier that helps the model to converge (to be guided in the right direction). 

![[Pasted image 20241105182759.png]]

# Residual networks

In 2015 **ResNet was introduced**: it was a real revolution towards **depth** (152 layer for ResNet). Once Batch normalization was introduce the researchers tried to go deeper, but the deeper model performed poorly. The initial guess was overfitting but the behavior was strange.
The real reason behind this trend is **optimization**: Deeper model are **harder to optimize** and in particular they don't learn the **identity function** to emulate shallow models.

The solution to this is let the network know that **some layers are not needed**: we add what are called **skip connections** (shortcut).
Networks are now composed of **residual block**: the model itself need to learn if it has to use a certain **stage**.

![[Pasted image 20241105183747.png]]

ResNet also implements the idea from GoogLeNet of **aggressive stem** to reduce the initial workload. It also implement a **global average pooling and a fully connected layer**.

**Basic vs Bottleneck** residual block:

![[Pasted image 20241105184034.png]]

Nowadays **ResNet** is the standard and still widely used!
Model ensembles is widely used also.
# Summary

Early works in CNN shows that **bigger networks works better**: in particular with AlexNet, ZFNet and VGG.
GoogLeNet shift the attention on **efficiency** with all its characteristics.
ResNet showed us how to train extremely deeper networks.
Nowadays there are tiny networks for mobile devices.

**Neural Architecture search** aims to automate architecture design: kind of a meta-optimization for networks to auto-choose which architecture to use.

