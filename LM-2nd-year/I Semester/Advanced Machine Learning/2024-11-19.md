# Object Detection

So far, we've seen architectures for image classification. A different kind of task is the **Object Detection**.
The input of this task is **one single image**, we want to return a **set of detected objects**, and for each detected object we would like to predict the **category label** (fixed) and the **bounding box**, represented by a 4-values tuple: (x, y, width, height).

![[Pasted image 20241119161525.png]]
Note that we have both multiple types of output and multiple outputs too.
For this task we **need images with higher resolution**.

### Multi-task Learning vs Transfer Learning

Since we need to solve two problem (where and what) we are talking about **multi-task learning**: let the model learn two task at a time. 

This concept is in contrast to **transfer learning**: you make the model learn something, then you reuse the knowledge for some other task.

Formalizing the concept of multi-task learning, we can think of the matrix $W$ as the combination of single $w_{t}$ matrices for each different task:
$$
W*=arg min \sum_{t} l(D_{t},w_{t}) + \gamma \lvert W \rvert^2_{F} 
$$
Doing this way we are just summing up the different tasks, but they are not interacting to help each other. In an old paper it was proposed to use a **trace norm** as a regularizer.
This has the effect of having a preferred low-rank solution of $W$.
Since we are asking for a regularization on rows (they have to be optimal for all tasks), we are imposing collaboration between tasks.
In other words, Multi-task learning biases the model to prefer representations that other tasks also prefer.
- In this way, MTL can be seen as a form of regularization

### Detecting a single object

Starting from a conventional network we could try adding **two heads**: one for classification and the other for the bouncing box.
In particular the second head, should be a fully connected layer with 4 dimensions as output. These are the **predicted box coordinates**: they will be compared with the correct values (regression problem).
The loss from the two heads are then combined into a single loss.

![[Pasted image 20241119163211.png]]

In general we said that it is not a good idea to sum the losses. But in this case we are imposing collaboration between **tasks because they share the same convolutional backbone**. Often the convolutional backbone is trained on ImageNet, so we are using transfer learning.
If the offset between losses is big, then we could lose meaning in the loss: it could be useful to weight the losses.

### Detecting multiple objects

If we need to detect multiple objects, then we need multiple outputs (not known a priori). **Each image requires a different number of outputs**.

A first approach to detect multiple objects is thinking the problem as classification, implementing a **sliding windows**: we apply a CNN to many different crops of the image, then CNN classify each objects as object or background.
However the high number of possible crops makes the approach infeasible.
$$
\text{Possible crops} = \frac{H(H+1)}{2} \frac{W(W+1)}{2}
$$

To avoid this problem we could use **algorithms that find image regions that are likely to contain objects**. An example is Selective search.

![[Pasted image 20241119164032.png]]

### R-CNN

These models are **Region-based CNN**:

![[Pasted image 20241119164217.png]]

Note that we are using the value that the CNN predicts as a parameter to compute the correct position of the box. We are doing this way, because we have an intermediate warping, that invalidates the 1 to 1 relation in images. (we are converting always into 224 x 224).

### Evaluation boxes

A first metric to evaluate boxes is **Jaccard similarity**, which is **Intersection Over Union (IoU)**:
$$
\text{IoU} = \frac{\text{Area of intersection}}{\text{Area of union}}
$$
IoU values over 0.5 are considered decent, above 0.7 is very good, near 0.9 is almost perfect.

![[Pasted image 20241119165440.png]]

Detectors tends to predict many overlapping detections.
If we have too boxes overlapping we could use **Non-Max-Suppression** (NMS):
1. Select next highest-scoring box
2. Eliminate lower-scoring boxes with a IoU below a specific threshold
3. If any boxes remain, GOTO 1

![[Pasted image 20241119165320.png]]

It is possible for NMS to **eliminate good boxes when objects are highly overlapping**, so it does not provides good solutions.

Another metric is **Mean Average Precision** (mAP):
1. Run object detector on all test images with NMS
2. For each category, compute Average Precision (AP), which is Area under Precision vs Recall curve
	1. For each detection (sorted by score, higher first)
		1. If it matches some GT box with IoU > 0.5, mark it as positive and eliminate the GT
		2. Otherwise mark it as negative
		3. Plot a point on PR Curve
	2. Area under the curve is the AP
3. mAP is computed as the average of APs for each category 

| .                                    | .                                    | .                                    |
| ------------------------------------ | ------------------------------------ | ------------------------------------ |
| ![[Pasted image 20241119165751.png]] | ![[Pasted image 20241119165809.png]] | ![[Pasted image 20241119165825.png]] |

To hit the AP = 1.0 for each category, we need to:
- Hit all GT boxes with IoU > 0.5
- Have no "false positive" detections ranked above any "true positives"


### Fast R-CNN

Since now, with a R-CNN we need to do many forward passes. To speed up the process we could run the CNN once, before the warping.

![[Pasted image 20241119170417.png]]

Doing this, most of the work is done in the "backbone", while the per-region network is relatively lightweight.
There is a new problem now, how can we "crop" features (thinking as a tensor)? The goal is the following: 

![[Pasted image 20241119170807.png]]

But, since the dimensions are different we need to perform some calculations.
Techniques such as **RoI Pool** makes different-sized sub-regions (weird) and has slight misalignment.

Other techniques such as **RoI Align** uses **bilinear interpolation**:

![[Pasted image 20241119171057.png]]

Performance R-CNN vs Faster R-CNN:

![[Pasted image 20241119171214.png]]

To avoid the extra cost of region proposals, we could try to make the region "learnable", embedding the regions into the CNN.

### Fast*er* R-CNN

We insert **Region Proposal Network (RPN)** to predict proposals from features.

![[Pasted image 20241119171630.png]]

The RPN works as a brute force approach, where we take a set of **anchor box**.
How to choose the parameters of boxes? They can be chosen by hand, we could use K different anchor boxes at each steps, also we could choose the shape (rectangular, square...)

First stage:
- Run once per image
- Backbone network
- Region proposal network

Second stage:
- Run once per region
- Crop features
- Predict class
- Prediction bounding box offsets

Do we really need two stages?
It's possible to do a **single stage object detection**.

![[Pasted image 20241119174234.png]]

### Recap

Biggest **trade-off is performance vs speed**: double stage networks perform quiete good but they are slower compared to single stage networks, but they have lower accuracy.
Of course we could increase the size of the backbone, but this leads to slower networks.
Also we have diminishing returns for slower methods

![[Pasted image 20241119174521.png]]

This results are for 2016, nowadays we can train longer, we cloud have multi-scale backbone and better backbone indeed.

![[Pasted image 20241119174630.png]]

# Semantic Segmentation

The goal of **semantic segmentation** is to **predict a label for each pixel in the image**.
Note that semantic segmentation does not distinguish between instances of objects while classifying.
Usually starting from a sliding windows we immediately know that it is inefficient because it is not reusing shared features between overlapping patches.

A **Fully Convolutional approach** has a bunch of convolutional layers to make prediction for pixels **all at once!**.

![[Pasted image 20241119175451.png]]
However this approach has two problems:
1. Effective **receptive field size** is linear in number of convolutional layers: to correctly perform classification we need a lot of layers, so that i have a big receptive fields. In this way i consider the shape of the entire object when choosing the label for a pixel. This is complex and expensive
2. **Convolution is expensive** on high resolution images

To mitigate the problems we could use **downsampling and upsampling inside the network**.

![[Pasted image 20241119180019.png]]

So now we know how to do downsampling, (see AlexNet...), but how do we do upsampling?
We could use **un-pooling** or **bilinear interpolation** (see slides).
Also the **max un-pooling** requires the networks to remember which position had the max:

![[Pasted image 20241119180301.png]]

The **learnable upsampling** is based on the **Transpose convolution** (up-convolution) that makes the size of the output bigger w.r.t. the input. On 1-D example, this operation is the following:

![[Pasted image 20241119180743.png]]

# Instance Segmentation

In this task we try to identify **all the instance** of objects, while classifying **all pixels in an image**.

![[Pasted image 20241119181209.png]]

The approach is to perform object detection, then applying a **segmentation mask** and keeping only "**things**" (objects that can be enumerated).

Other tasks beyond this are:
- **Panoptic segmentation**: Label all pixels in images (both things and stuff, with instances)
- **Human keypoints**: Represent the pose of a human by locating a set of **keypoints**.
- Joining instance segmentation and pose estimation
- Dense captioning
- 3D shape prediction


# Supervised vs Unsupervised Learning

![[Pasted image 20241119182232.png]]

Why Unsupervised learning could be useful: that's because labeling data is costly, and using unsupervised approach our goal is to understand the structure of real world.

In particular, now we focus on **self-supervision learning**, which is a technique of unsupervised learning. We remove part of information from data, then we train the model to predict the missing part of data.
For example we could ask a model to predict the relative positions of two tiles from an image.

![[Pasted image 20241119182632.png]]

There is no need of labeling, because the information is contained itself inside the image.

A popular method is **contrastive learning**: is an approach that focuses on extracting meaningful representations by contrasting positive and negative pairs of instances. It leverages the assumption that similar instances should be closer in a learned embedding space while dissimilar instances should be farther apart. By framing learning as a discrimination task, contrastive learning allows models to capture relevant features and similarities in the data.

# Generative models

By nature, they are an **unsupervised** technique, that aims to **learn a probability distribution** of a bunch of data.
Then, sampling from the distribution, we could **generate** new images.
Different approaches:
- Explicit density estimation: explicitly define and solve for $p_{\text{model}}(x)$
- Implicit density estimation: learn model that can sample from $p_{\text{model}}(x)$  w/o explicitly defining it

![[Pasted image 20241119183440.png]]

# PixelRNN

This model is able to **generate image pixels starting from a corner**. The dependency of a pixel with the previous ones is exploited with a Recurrent Neural network.

![[Pasted image 20241119183957.png]]

One problem with this model is efficiency: **sequential generation is slow**.
Pros are: can explicitly compute the likelihood, explicit likelihood gives a good evaluation metrics

# Variational Autoencorders

Some background: an **autoencoder** is an unsupervised approach whose goal is to **recreate the input** (reconstructor)

![[Pasted image 20241119184516.png]]





