# PAC Learning

### True vs empirical risk

### Loss

### Minimization of risk

We define the **true risk** of a classifier $h$ as the probability that it does not predict the correct label on a random sample drawn from $\mathcal{D}$:
$$
R_{\mathcal{D}} (h ) = P(h(X) \neq f(X))
$$
In other words we can define the true risk as the **expected value of the loss**:
$$
R_{\mathcal{D}}(h) = E_{X \sim \mathcal{D}}(L(h(X), f(X)))
$$
Even when $R_{D}(h^*) = 0$ we can't hope to find $h$ with $R_{D}(h) = 0$
So we would like that our $h$ has the following risk: $R_{D}(h) \leq \epsilon$
We also have no guarantee that this will happen, so we will allow our model to fail with a probability $\delta \in (0,1)$.
These concepts are fundamental in every learning process known as **PAC learning**:

![[Pasted image 20241011120145.png]]

$m_{H}$ is called **sample complexity**: this highlights the fact that we need more observation that a certain threshold, defined by $m_{H}$.

# Underfitting / Overfitting

![[Pasted image 20241011120918.png]]

This plot describes the errors generated by the model w.r.t. the capacity (numbers of parameters) of the model. Increasing the number of parameters can lead to overfitting situations.
It means that our predictor has excellent performance on training set, but performs poorly in the real world.

### Cross validation

![[Pasted image 20241011121249.png]]

Leave-one-out approach is recommended when working with a small dataset.

### Regularization

A possible strategy to avoid overfitting is the **regularization**: we introduce a penalty for the model complexity, in other words we can control the complexity.
In the Empirical Risk minimization we add a $\lambda$ regularization term:

![[Pasted image 20241011122227.png]]

$\lambda$ is a hyper-parameter, it means that it needs to be tuned based on cross-validation.

# Linear Regression

Since the output of LR is a continuous number, we are using a squared error $L(\hat{y},y) = (\hat{y}-y)^2$
