# Logistic regression

Loss of logistic regression, casted as a cost function:

![[Pasted image 20241015161840.png]]

Where $\sigma(x) = \frac{1}{1+e^{-x}}$ is the **sigmoid function**.
This is called **logistic loss**, that is different from the **hinge loss** (used in SVM).
To minimize the loss we need to apply the first-order condition:
$$
\nabla_{\Theta}l_{\Theta} = 0
$$
![[Pasted image 20241015162657.png]]

We now focus on this term (3 concatenated functions) and if we solve this we get a solution depending on both $a$ and $b$. We need to do calculate all derivatives w.r.t. all variables.
We notice that **the parameters enter the gradient in a non linear way**.
Thus $\nabla_{\Theta}l_{\Theta}= 0$ is not a linear system and it is also a **transcendental** equation (no analytical solution).
We need to find another approach to optimize our loss.

### Gradient descent

Our approach will be **gradient descent**: looking at the **loss landscape** we iteratively move in the direction where the function decreases the most.

![[Pasted image 20241015163611.png]]

In other words we compute at each time step $t$:
$$
\Theta^{(t+1)} = \Theta^{(t)} - \alpha \cdot \nabla l_{\Theta (t)}
$$
This approach requires that **function is differentiable**.
Also, gradient descent **may remain stuck in stationary point**, but they are not local minima.
The $\alpha$ parameter is called **learning rate**: being able to select this parameter in an accurate way may lead to efficiency.
- If $\alpha$ is small we will have slow convergence
- If $\alpha$ is too big we will have overshooting
- Selecting the correct $\alpha$ allows us to reach the local minimum in a certain time

The learning rate can be **adaptive**:
1. We can decrease $\alpha$ to a **decay parameter** $\rho$
2. We can accumulate past gradients and keep moving in their direction ![[Pasted image 20241015165403.png]]With the first equation we are accumulating the gradient at each step (momentum), then with the second equation we are moving in a direction. The more the gradient is pointing to a certain direction, the faster we will move.

We can also apply gradient descent to non-convex problem, but there are no guarantee that we find a minimum. However, to gain generalization, we are rarely interested in a global optimum. (it means overfit the model)
We are more interested in local optimum, more efficient and computationally feasible.

### Stochastic gradient descend

There are limitations to classic gradient descend: the number of samples and the number of parameters can be very high. To avoid this problem we can use a **mini-batch** extracted from the training sample to approximate the gradient. Even if the mini-batch is small, it should be representative for our data.
After applying this approach, the gradient is approximated, but with a **significantly speedup**.

We can summarize the algorithm in the following steps:
1. Initialize $\theta$
2. Pick a mini-batch $\mathcal{B}$
3. Update with the downhill step $\theta \leftarrow \theta - \alpha \nabla l_{\theta}(\mathcal{B})$ 
4. Go back to step 2
Once you cover the entire training set we have an **epoch**.

Pros of this approach: Oscillations introduces randomness that allows us to avoid overfitting and also avoid local minimum.
Also, running the algorithm for different epochs avoid stopping in a local minimum.

Practical consideration with SGD:
- We need to shuffle data to avoid data bias
- Each mini-batch can be processed in parallel
- If a dataset is large, we will have initial progress and then a slow asymptotic convergence
- Small mini-batches can offer a **regularization** effect.

# Neural networks

### Perceptron

Basic component: **perceptron**, which is a single component that acts as a weighted sum of inputs and weights, then a "step function" is applied.
If we call $w$ the weights vector and $x$ the inputs, we can describe the perceptron as:
$$
h(x_{i}) =\text{sign}(w^Tx_{i}+b)
$$
Perceptron can be used as classifier with the following algorithm:
1. Start with weights = 0
2. For each training instance we classify a sample
3. If we are correct we do nothing, otherwise we update the weights using this formula: $w=w+y^* \cdot x$
If data are linearly separable, then the perceptron will commit at most $\frac{1}{\gamma^2}$ mistakes, where $\gamma$ is the "margin"

![[Pasted image 20241015175613.png]]

Problems with perceptron:
- poor generalization
- overfitting
- If data are not linearly separable, then we get trash

To **improve the perceptron** we could add a non-linearity to provide more "probabilistic" output: for example, instead of the step function we could use a sigmoid.

### Deep ReLU network

Stacking ReLU function as the output of layers, we can approximate any complicated function, to model real world.

This concept above is generalized in the following theorem:

![[Pasted image 20241015182534.png]]

These type of theorem are not constructive, i.e. they do not tell us how to find these approximation, but they guarantee the existence.

### Training

In general we consider the MSE as the loss for the training process. However the loss in not convex w.r.t. the parameters $\Theta$. We can identify special cases:
- If we have one layer, no activation, MSE as loss -> Linear regression
- If we have one layer. sigmoid activation, logistic loss -> Logistic regression

In general we are dealing with a combination of linear and non-linear.
In order to optimize this model we need to compute gradients, but computationally that is non feasible. Even if we try to use finite differences (definition of derivative) it is not feasible. So we need another strategy, called **backpropagation**, which basically uses the chain rule.
Considering a function we try to compute a **computational graph**, which is basically an acyclic directed graph that describe the steps to compute a mathematical expression with all intermediate steps.

![[Pasted image 20241022161923.png]]

The evaluation of a function $f(x)$ corresponds to a forward traversal of the graph.

We can now compute the derivatives of all nodes based on the corresponding inputs.
Doing this way, the cost of computing the derivative of $f(x)$ is the same of computing $f(x)$ itself, because we are composing it with little pieces.

![[Pasted image 20241022162519.png]]

However we are dealing with multi-dimensional input functions $f: \mathcal{R}^p \to \mathcal{R}$
So the cost of computing the **gradient of $f(x)$** is equal to:
$$
\text{cost of } \nabla f(x) = p \times \text{cost of computing } f(x) 
$$
The approach described so far is known as **forward mode**, because we are traversing the graph and computing all partial derivatives w.r.t. the input $x$ (i.e. straightforward application of chain rule). This method accumulates numerical derivative evaluations rather than expression. 

However we need to change the mode of the problem, by computing alla derivatives of $f(x)$ w.r.t. to **inner nodes**: this is called **reverse mode**.

![[Pasted image 20241022163310.png]]

Since we need internal nodes values, we need to:
1. Make a **forward pass**, where we ONLY calculate the function values of internal nodes
2. **Backward pass** to compute derivatives

When dealing with loss function $l: \mathcal{R}^p \to \mathcal{R}$ where $p$ is the number of weights, we don't need to calculate simple derivatives, but instead we **need to calculate gradients and Jacobians**. 
It can be proved that:

![[Pasted image 20241022163801.png]]

So the reverse mode does not have that $p$ factor that would make the calculation non feasible.

This all logic is called **back-propagation**, which is "reverse mode automatic differentiation" applied to deep neural networks. With this approach, evaluation the loss $l$ is as fast as evaluating the gradient $\nabla l$.

Some observations on the loss function:
- Loss function is not convex in general, so we will be satisfied with a local minima.
- Searching for a global minimum generally tends to overfit the model
- In general there are multiple minima, and the one reached depends on **weight initialization**.
- In general, the loss is **non-differentiable**, however software returns one of the one-sided derivatives
- Numerical issues are always present

So training a network means looking for a minima for the loss, however there is no correct way of training a network.




