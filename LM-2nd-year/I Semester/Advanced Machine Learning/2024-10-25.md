# Batch normalization

During the evolution of convolutional network, a new part of architecture was introduced: **Batch normalization**.
Each layer takes as input the output of the previous layer, this means that we can get bigger values with wider range at each step. To simplify the calculation we introduce a **normalization** to get outputs with zero-mean and unit covariance.
$$
\hat{x} = \frac{x- E[x]}{\sqrt{ \text{Var} [x]}}
$$
This helps reducing internal covariance shift and improves optimization.
In particular we can compute the empirical mean and variance independently for each dimension.
Note that the function described above is a **differentiable function**, so it can be used as an operator and back-propagated it.
Also this process is done for every batch: this helps introducing some randomness and regularizing the model.

The network could have improvements with the normalization process, however we also want to give the network the **freedom to go back to the original $x$ values**:
$$
y=\gamma \hat{x} + \beta
$$
![[Pasted image 20241025114842.png]]

The benefits of normalization are:
- **Improves gradient flow** through the network
- Allows **higher learning rates**
- Reduces the strong dependence of initialization
- Acts as a **form or regularization**

This process is valid at **training time**.
During **test time**: the mean and std are not computed on batch. Instead a single fixed empirical mean and std are used (computed, for example, during training time)

A variant of batch normalization could be **layer normalization**, but also **instance normalization**.

![[Pasted image 20241025115802.png]]

# Activation functions

![[Pasted image 20241025115945.png]]

**Sigmoid**, $\sigma(x) = 1/(1+e^{-x})$ squashes numbers to range $[0,1]$, was historically popular. However we can identify 3 problems:
- The "plateau" at the ends of the function **"kills" the gradient** (neurons are saturated)
- The outputs are **not zero-centered** (all values are positive) ![[Pasted image 20241025120642.png]]
-  The function contains an **exponentiation** which is costly

**Tanh** squashes number to the range $[-1,1]$, but still **kills the gradient**.

The **ReLU** function does **not saturate**, it's very computationally efficient, converges much faster and it's also more biologically plausible. However we are dealing with 2 problems: **No zero-centered output** and what about the gradient when $x <0$? We could end up in a situation called "**dead ReLU**" that will never update.

A possible solution to this could be use a **leaky ReLU** or a **parametric ReLU**:

![[Pasted image 20241025121244.png]]

One possible problem could be the angular point in $x=0$, to solve this problem we could use **exponential linear units** (**ELU**):

![[Pasted image 20241025121401.png]]

In practice we always use ReLU, we could try a leaky ReLU if you want to obtain an extra 0.1%, but in general try not to use sigmoid on tanh.

# Data pre-processing

So far, we have seen all components needed to build a network. Let's now focus on the data itself that will be feed to the network.

We could apply **PCA and data whitening**.

In general, the most useful transformation is **data normalization**, which can help with optimization, makes data less sensitive to small changes. We could also apply other transformation such as decorrelation, whitening...

For **images** it's always useful to:
- **Subtract the mean** images
- Subtract **per-channel mean**
- Subtract **per-channel mean** and **divide per-channel std**
However it is **not common** to do PCA or whitening.

# Weight initialization

This is another common problem when training a network. We could try different strategies.
For example we could start from **small values** with zero mean and $0.01$ standard deviation.

```python
W = 0.01 * np.random.randn(Din,Dout)
```

This approach works for small networks, but it does not scale for larger networks.
That is because the std tends to zero when passing through activation layers (tanh).

A more useful approach could be using the **Xavier initialization**:

```python
W = np.random.randn(Din, Dout) / np.sqrt(Din)
```

This choice allows to **maintain the variance of the inputs the same as the variance of the outputs**.

If we are working with a ReLU activation, then Xavier initialization fails because weights collapse to zero.

# Regularization

### Dropout

First technique to perform regularization is to add a penalty term.
Another technique is **dropout**: in each forward pass, randomly set some neurons to zero. Probability of dropping is a **hyperparameter**, for example 0.5 is common.
This introduces some randomness into the network: it forces the network to have a **redundant representation**, it also **prevents co-adaptation** of features.

Dropout during test time, requires to multiply with its probability:

![[Pasted image 20241025124502.png]]


### Data augmentation

We could introduce "Noises" directly in the input. For images we could apply cropping, horizontal flips, color jitter, change brightness, and more complex one.
A particular transformation si **mixup** where we basically average two images together 

![[Pasted image 20241025125208.png]]

My goal for the output will be 0.6 for dog and 0.4 for cat

# Babysitting the learning process

During gradient descent process we are dealing with an important **hyper parameter** which is the **learning rate** (i.e. the step of the gradient descent).

![[Pasted image 20241105161339.png]]

If the learning rate is **extremely** **high**, the loss will diverge.
A **High** learning rate means that initially decrease but it remains high.
A **low** learning rate allows the model to converge but it will take lots of time.
A **Good learning rate** provides the best results, the model converges and we get a low loss
This parameters requires **cross validation**, however a good technique is to try different values, maybe starting with a large value and then decay over time.

This process is called **Learning rate decay**: a possible schedule is to reduce the loss when we can notice a sort of plateau:

![[Pasted image 20241105161811.png]]

Another approach is to use a **fixed schema**, like decreasing the loss after epoch 30,60,90 out of 100 epochs.
We could also use a **cosine function** to define the decay of the learning rate:
$$
\alpha_{t} = \frac{1}{2} \cdot \alpha_{0}\left( 1+\cos\left( \frac{t\pi}{T} \right) \right)
$$
We could also use **linear decay** or **inverse sqrt**, and also use a constant!

```ad-note
title: Strange behavior vs Learning rate
If a model has not the correct behaviour, that does not depend on the learning rate. The learning rate only influence performance in terms of time of convergence.
```

Obviously the more epoch we train, the more the accuracy will increase, however that could lead to a **overfitting problem**. To overcome this we can use a validation set and we could stop the training when the accuracy **on the validation decreases**. This is **always a good idea**.

![[Pasted image 20241105162543.png]]

To do this, we could take a **snapshot** of the model (for example of each epoch) and look at the validation set.

# Hyperparameter optimization

Let's consider we need to optimize two hyperparameters: the learning rate and the **weight decay** ($\lambda$, useful for regularization).
The first option to optimize is the **grid search**: we try all possible combinations out of a set of possible values.
An improvement could be analyzing **range of values**, for example sampled from a normal distribution: approach called **random search**.

![[Pasted image 20241105163147.png]]

We can see that with "fixed grid structure" we have no clue on how the loss behaves w.r.t. the hyperparameters. With the random approach we have a better overview of the behave of the function.

# Choosing hyperparameters

1. At the beginning we decide to turn off any kind of regularization. We **train the model and we check the initial loss**. Note that at the beginning we expect the model to perform according to our approximation (around $\log(C)$, where $C$ is the number of class)
2. We **overfit a small sample**: this means that the model is behaving correctly and everything works (like 5 or 10 mini batches). If the model cannot overfit (loss not going down) this means that there are bugs or the LR is too low. If the loss grows to infinite this means again bugs or LR too high.
3. So we need to figure out a Learning rate that makes the loss go down. Use the architecture defined in the previous step, use all training data and introduce a small weight decay.
4. Choose a few values for LR and weight decay and train the model for 5-10 epochs
5. Refine the grid, searching for the best values 
6. Look at the learning curves

![[Pasted image 20241105164229.png]]

A possible problem could be **bad initialization of LR**: this can be translated in a first plateau in the learning curve. If there is a plateau then the weight decay needs a new value.
If there is a step down it means that the decay happened too early.

Regard the validation and training:

![[Pasted image 20241105164510.png]]

![[Pasted image 20241105164531.png]]

![[Pasted image 20241105164542.png]]

An other thing we need to check is the **ratio between the weight update and weight magnitude**:

```python
param_scale = np.linalg.norm(W.ravel())
update = -learning_rate*dW
update_scale = np.linalg.norm(update.ravel())
W += update
print(update_scale / param_scale)
```

We want this **measure to be near $0.001$**

