# The need of priors

Deep feed-forward neural networks are provably **universal**, however we can make them arbitrarily complex (number of parameters can be huge) and making them difficult to optimize and achieve generalization.

We need to introduce some **priors** as a remedy to the problem described above.
Data can have particular features that can be extracted and used to **guide a model** to perform a specific task. In particular this can be well visualized with images.

We start with a **key insight**: data often carries **structural priors** in terms of repeating patterns, compositionality, locality.
Data also to be **self-similar** (another prior) across the domain: for example different colors on completely different objects.

![[Pasted image 20241022165920.png]]

For example, removing the bird, a model could be still able to recompose the image of the fence, with only existing pieces of the remaining image, due to pattern repetition.
Also it's important to know that **translation invariance is desirable across multiple domains**: an object is still the same object regardless of its position.
Another kind of information of images is **hierarchy and compositionality**.

# Convolutional neural networks

As described above, images (data) is often composed of hierarchical, local, shift-invariant patterns. A CNN **directly exploits this fact as a prior**.

![[Pasted image 20241022170510.png]]

We are applying the convolution operator into a **convolutional layer**: we talk about **weight sharing** because the outgoing edges have the same weights for each "window".

```ad-note
title: Convolution
Given two function $f,g: [-\pi,\pi] -> \mathcal{R}$, their convolution is a function:
$$
(f * g)(x) = \int_{-\pi}^{\pi} f(t) g(x-t) dt
$$
Convolution is a **commutative and shift-equivariant operator**.
```

On images, we can think the convolution as **applying a filter** on them.
On 2-D domains, with RGB $R^2 \to R^3$ then we can compute convolution as:
$$
(f*g)[m,n] = \sum_{k} \sum_{l} f[k,l]g[m-k,n-l]
$$
We can visualize the convolution as a **sliding windows** (square) moving on the image. 

![[Pasted image 20241022171459.png]]

The new parameters of this networks will be the value of the sliding window elements.

![[Pasted image 20241022171715.png]]

For simplicity imaging the filter as a 3 x 3 grid that is applied to all the image: in this case the weight of each cell will be applied to the image: so now the concept of **weight sharing** makes sense.
In first layer, a network is dealing with larger portions of input, so we do not have a useful view of the interactions in the image
The pooling stage is helpful because it allows to **capture non-local interaction** via simple building block that **only describe sparse interactions.**
Pooling can be exploited via many strategies: a simple one can be max pooling.

![[Pasted image 20241022172329.png]]

So, CNN exploits the following features:
- Convolutional filters (Translation equivariance)
- Multiple layers (Compositionality)
- Filters localized in space (locality)
- Weight sharing (Self-similarity)

Let's consider a 32 x 32 x 3 image as input of a convolutional layer, and a filter of size 5 x 5 x 3. We can **convolve** the filter with the image.

![[Pasted image 20241022175010.png]]

The effect of the filter is getting a new matrix of size 28 x 28, which is called **activation map**. Of course we can apply more than one filter to the original image. We can horizontally stack the activation maps to get a "new image" which will be fed to the new layer

![[Pasted image 20241022175338.png]]

A popular example: ConvNet is a sequence of Convolutional Layers, interspersed with
activation functions (example ReLU) .

In general, if we are working on a $N\times N$ image, with a $F \times F$ filter and a stride $S$ (how much we move the filter at each step), a convolutional layer will create an activation map of:
$$
\frac{N-F}{S} +1
$$
Of course, not all possible strides are allowed, however it is possible to apply a **zero-padding** to the input image, so we can choose any $S$ value.

Another concept we need to introduce is the **receptive field**: each element in the output depends on a $K \times K$ area in the input. With this notation, the filter is called **kernel** and its size is $K \times K$.

![[Pasted image 20241022180818.png]]

Note that a 1 x 1 filter makes perfect sense, because it is useful **to reduce output depth**.

```ad-example
title: Output volume size?
Input volume: 32x32x3
10 5x5 filters with stride 1, pad 2

Remember that, if we are working with padding we need to double-add it to the input size ($N=N+2*P$)

Output volume size is: (32+2*2-5)/1 +1 = 32 spatially, so 32x32x10

Number of parameters in this layer: each filter has 5x5x3 +1 = 76 parameters, so for 10 filters we have 10*76=760 parameters

Numbers of multiply-add operations?
10x32x32 outputs, each output is the inner product of two 5x5x3 tensor, for a total of 10*32*32*5*5*3= 768k
```

A quick recap:

![[Pasted image 20241022181621.png]]

The **pooling** makes the representation smaller and more manageable. The pooling function is an **hyperparameter** (so the model cannot learn it, such as the kernel size and the stride).
The pooling operates on each activation map independently.

![[Pasted image 20241022181851.png]]

